{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers import Dropout, Flatten, Input, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuraçoes\n",
    "\n",
    "FAST_RUN = True\n",
    "IMAGE_WIDTH=28   \n",
    "IMAGE_HEIGHT=28\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=1\n",
    "NUM_CLASS = 2 #numero de classes para prever\n",
    "PATIENCE_ES = 10 #paciencia do early stop\n",
    "EPOCH_TRAIN = 20 #quatidade de epocas do treinamento  \n",
    "TEST_VAL_SIZE = 0.1 #percentual dos dados usados pra validaçao\n",
    "SIZE_BATCH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              label   pixel0   pixel1   pixel2   pixel3   pixel4   pixel5  \\\ncount  42000.000000  42000.0  42000.0  42000.0  42000.0  42000.0  42000.0   \nmean       4.456643      0.0      0.0      0.0      0.0      0.0      0.0   \nstd        2.887730      0.0      0.0      0.0      0.0      0.0      0.0   \nmin        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \nmax        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n\n        pixel6   pixel7   pixel8  ...      pixel774      pixel775  \\\ncount  42000.0  42000.0  42000.0  ...  42000.000000  42000.000000   \nmean       0.0      0.0      0.0  ...      0.219286      0.117095   \nstd        0.0      0.0      0.0  ...      6.312890      4.633819   \nmin        0.0      0.0      0.0  ...      0.000000      0.000000   \n25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n75%        0.0      0.0      0.0  ...      0.000000      0.000000   \nmax        0.0      0.0      0.0  ...    254.000000    254.000000   \n\n           pixel776     pixel777      pixel778      pixel779  pixel780  \\\ncount  42000.000000  42000.00000  42000.000000  42000.000000   42000.0   \nmean       0.059024      0.02019      0.017238      0.002857       0.0   \nstd        3.274488      1.75987      1.894498      0.414264       0.0   \nmin        0.000000      0.00000      0.000000      0.000000       0.0   \n25%        0.000000      0.00000      0.000000      0.000000       0.0   \n50%        0.000000      0.00000      0.000000      0.000000       0.0   \n75%        0.000000      0.00000      0.000000      0.000000       0.0   \nmax      253.000000    253.00000    254.000000     62.000000       0.0   \n\n       pixel781  pixel782  pixel783  \ncount   42000.0   42000.0   42000.0  \nmean        0.0       0.0       0.0  \nstd         0.0       0.0       0.0  \nmin         0.0       0.0       0.0  \n25%         0.0       0.0       0.0  \n50%         0.0       0.0       0.0  \n75%         0.0       0.0       0.0  \nmax         0.0       0.0       0.0  \n\n[8 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>42000.000000</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>...</td>\n      <td>42000.000000</td>\n      <td>42000.000000</td>\n      <td>42000.000000</td>\n      <td>42000.00000</td>\n      <td>42000.000000</td>\n      <td>42000.000000</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n      <td>42000.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.456643</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.219286</td>\n      <td>0.117095</td>\n      <td>0.059024</td>\n      <td>0.02019</td>\n      <td>0.017238</td>\n      <td>0.002857</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.887730</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6.312890</td>\n      <td>4.633819</td>\n      <td>3.274488</td>\n      <td>1.75987</td>\n      <td>1.894498</td>\n      <td>0.414264</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>254.000000</td>\n      <td>254.000000</td>\n      <td>253.000000</td>\n      <td>253.00000</td>\n      <td>254.000000</td>\n      <td>62.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 785 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA FOR NEURAL NETWORK\n",
    "Y_train = train[\"label\"]\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = test / 255.0\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE MORE IMAGES VIA DATA AUGMENTATION\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  \n",
    "        zoom_range = 0.10,  \n",
    "        width_shift_range=0.1, \n",
    "        height_shift_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 3\n",
    "model = [0] *nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
    "j=0\n",
    "model[j] = Sequential()\n",
    "\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 5, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "model[j].add(Conv2D(64, kernel_size = 4, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Flatten())\n",
    "model[j].add(Dropout(0.4))\n",
    "model[j].add(Dense(10, activation='softmax'))\n",
    "\n",
    "# COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n",
    "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
    "j=1\n",
    "model[j] = Sequential()\n",
    "\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 5, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Flatten())\n",
    "model[j].add(Dropout(0.4))\n",
    "model[j].add(Dense(10, activation='softmax'))\n",
    "\n",
    "# COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n",
    "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD CONVOLUTIONAL NEURAL NETWORKS\n",
    "j=2\n",
    "model[j] = Sequential()\n",
    "\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Dropout(0.4))\n",
    "\n",
    "model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "model[j].add(BatchNormalization())\n",
    "model[j].add(Flatten())\n",
    "model[j].add(Dropout(0.4))\n",
    "model[j].add(Dense(10, activation='softmax'))\n",
    "\n",
    "# COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n",
    "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n - 28s - loss: 0.6202 - accuracy: 0.8097 - val_loss: 0.1431 - val_accuracy: 0.9569\nEpoch 2/10\n - 24s - loss: 0.1604 - accuracy: 0.9543 - val_loss: 0.0566 - val_accuracy: 0.9829\nEpoch 3/10\n - 24s - loss: 0.1110 - accuracy: 0.9680 - val_loss: 0.0497 - val_accuracy: 0.9862\nEpoch 4/10\n - 24s - loss: 0.0914 - accuracy: 0.9738 - val_loss: 0.0674 - val_accuracy: 0.9814\nEpoch 5/10\n - 24s - loss: 0.0807 - accuracy: 0.9763 - val_loss: 0.0373 - val_accuracy: 0.9898\nEpoch 6/10\n - 24s - loss: 0.0699 - accuracy: 0.9792 - val_loss: 0.0424 - val_accuracy: 0.9881\nEpoch 7/10\n - 25s - loss: 0.0683 - accuracy: 0.9803 - val_loss: 0.0493 - val_accuracy: 0.9843\nEpoch 8/10\n - 24s - loss: 0.0635 - accuracy: 0.9813 - val_loss: 0.0282 - val_accuracy: 0.9917\nEpoch 9/10\n - 25s - loss: 0.0576 - accuracy: 0.9834 - val_loss: 0.0334 - val_accuracy: 0.9888\nEpoch 10/10\n - 25s - loss: 0.0533 - accuracy: 0.9851 - val_loss: 0.0347 - val_accuracy: 0.9893\nCNN 1: Epochs=10, Train accuracy=0.98513, Validation accuracy=0.99167\nEpoch 1/10\n - 31s - loss: 0.4488 - accuracy: 0.8624 - val_loss: 1.9547 - val_accuracy: 0.5495\nEpoch 2/10\n - 27s - loss: 0.1212 - accuracy: 0.9636 - val_loss: 0.0377 - val_accuracy: 0.9890\nEpoch 3/10\n - 27s - loss: 0.0875 - accuracy: 0.9738 - val_loss: 0.0723 - val_accuracy: 0.9781\nEpoch 4/10\n - 27s - loss: 0.0780 - accuracy: 0.9771 - val_loss: 0.0342 - val_accuracy: 0.9890\nEpoch 5/10\n - 27s - loss: 0.0673 - accuracy: 0.9809 - val_loss: 0.0416 - val_accuracy: 0.9879\nEpoch 6/10\n - 28s - loss: 0.0590 - accuracy: 0.9818 - val_loss: 0.0398 - val_accuracy: 0.9898\nEpoch 7/10\n - 27s - loss: 0.0561 - accuracy: 0.9836 - val_loss: 0.0324 - val_accuracy: 0.9900\nEpoch 8/10\n - 28s - loss: 0.0500 - accuracy: 0.9849 - val_loss: 0.0349 - val_accuracy: 0.9893\nEpoch 9/10\n - 27s - loss: 0.0433 - accuracy: 0.9876 - val_loss: 0.0320 - val_accuracy: 0.9905\nEpoch 10/10\n - 27s - loss: 0.0384 - accuracy: 0.9879 - val_loss: 0.0309 - val_accuracy: 0.9914\nCNN 2: Epochs=10, Train accuracy=0.98794, Validation accuracy=0.99143\nEpoch 1/10\n - 26s - loss: 0.4670 - accuracy: 0.8558 - val_loss: 0.3913 - val_accuracy: 0.8833\nEpoch 2/10\n - 21s - loss: 0.1317 - accuracy: 0.9600 - val_loss: 0.0374 - val_accuracy: 0.9895\nEpoch 3/10\n - 21s - loss: 0.1003 - accuracy: 0.9698 - val_loss: 0.0318 - val_accuracy: 0.9921\nEpoch 4/10\n - 21s - loss: 0.0839 - accuracy: 0.9738 - val_loss: 0.0422 - val_accuracy: 0.9881\nEpoch 5/10\n - 21s - loss: 0.0749 - accuracy: 0.9775 - val_loss: 0.0268 - val_accuracy: 0.9910\nEpoch 6/10\n - 21s - loss: 0.0629 - accuracy: 0.9815 - val_loss: 0.0303 - val_accuracy: 0.9907\nEpoch 7/10\n - 21s - loss: 0.0618 - accuracy: 0.9825 - val_loss: 0.0213 - val_accuracy: 0.9926\nEpoch 8/10\n - 22s - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0238 - val_accuracy: 0.9933\nEpoch 9/10\n - 21s - loss: 0.0536 - accuracy: 0.9835 - val_loss: 0.0242 - val_accuracy: 0.9936\nEpoch 10/10\n - 21s - loss: 0.0479 - accuracy: 0.9857 - val_loss: 0.0182 - val_accuracy: 0.9955\nCNN 3: Epochs=10, Train accuracy=0.98569, Validation accuracy=0.99548\n"
    }
   ],
   "source": [
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "# TRAIN NETWORKS\n",
    "history = [0] * nets\n",
    "epochs = 10 #45\n",
    "for j in range(nets):\n",
    "    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.1)\n",
    "    history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n",
    "        epochs = epochs, steps_per_epoch = X_train2.shape[0]//64,  \n",
    "        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=2)\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "        j+1,epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE PREDICTIONS AND SUBMIT\n",
    "results = np.zeros( (X_test.shape[0],10) ) \n",
    "for j in range(nets):\n",
    "    results = results + model[j].predict(X_test)\n",
    "results = np.argmax(results,axis = 1)\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "submission.to_csv(\"MNIST-CNN-ENSEMBLE.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38032bit4c9d6fc1c70e4c37971e95771c728caa",
   "display_name": "Python 3.8.0 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}